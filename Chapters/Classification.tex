\newacronym{LDA}{LDA}{Latent Dirichlet Allocation}
\newacronym{nltk}{NLTK}{Natural Language Toolkit}
\section{Classification}
\subsection{Overview}
While classification is only a single part of the CloudAssure system, all
operations hinge off its categorization decision. Classification is currently an
very active area of research \autocite{Zhang2000}, especially with respect to
internet documents. Classification is a branch of machine learning which
attempts to predict the underlying topic, or classification of a given document.
As a discipline under machine learning it's helpful to describe machine learning
at a high level, before describing the classification techniques evaluated and
finally implemented in CloudAssure
%TODO Add the classifiction overview diagram to this section

\subsection{Machine Learning with Respect to Classification}
Machine Learning can be described as a mathematical model which reverses a human
process \autocite{Bishop2009}. In the area of classification this is
a straightforward description. For example, a given document is composed of words which
intend to persuade or inform the reader (See
Figure~\ref{fig:LDA_Generative_Model}) Thus, the documents words are related
by some list of topics \autocite{Blei2009}. \gls{LDA} supposes that documents are
written in the following fashion:
\begin{enumerate}
    \item The author has a finite set of lists. Each list contains a vocabulary
        for a given topic.
    \item The author chooses to start writing a document.
    \item The author rolls a die to choose a topic list.
    \item The author rolls the die again to choose a word from the selected
        topic list.
    \item The author writes down the word.
    \item The author continues rolling the die until the document is of sufficient
        length.
    \item Once the document is complete, the author destroys the topic lists.
\end{enumerate}

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.90\textwidth]{Figures/LDA_Generation.png}
        \caption{Example of the LDA Generative Document Model}
        \label{fig:LDA_Generative_Model}
    \end{center}
\end{figure}
%TODO Insert the pretty colored document picture here.
Now given any document, what are the topics contained in it? The original topic
lists were destroyed, so \gls{LDA} proposes a method to recreate them. Thus reversing
the process of writing a document.

LDA is only one example, but all classification methods use some algorithm which assumes a specific
distribution to the document with the intent of extracting some unobservable
property. During the course of designing CloudAssure's classification module, we looked at a number of techniques
including \gls{LDA}, Neural Nets, Bayesian Inference, Decision Trees, and finally,
Support Vector Machines.

\subsubsection{Latent Dirichlet Allocation}
In ``Probabilistic Topic Models'', Blei describes the Latent Dirichlet
Allocation and its probabilistic approach to topic prediction
\autocite{Blei2012}. \gls{LDA} is different among the classical classifiers we
surveyed for CloudAssure. \gls{LDA} supposes documents are generated in
a mathematical way. Blei then uses this generative model to show how the original topic lists can be
recomposed from the document. This technique is very powerful for extracting
topics from a document, but for our usecase, there is a second step of
translating those topics to classifications. Due to this secondary step, we
choose not to use the LDA algorithm \autocite{RadimRehurek2010}.

\subsubsection{Neural Nets}
Neural Nets are another powerful classification technique. An artificial neural
net's power
extends from its non-linearity \autocite{Zhang2000}. A neural net is composed on
a number of nodes connected by a ``hidden'' layer. The input and output nodes
then filter features through this hidden layer which triggers specific outputs
thus classifying a document \autocite{Merkl}. The neural net is very good at
doing this however since the ``hidden layer'' is, by definition, unobservable, and
the results are not traceable (i.e. given a result one cannot easily trace to
find the series of decision which lead to that decision). This increases the
complexity of implementation and debugging. Secondly, neural nets are sensitive
to their input data, more so that other techniques, thus a great deal of time
needs to be spend to tune the classifier. For these two reasons, we choose not
to implements the Neural Net technique for CloudAssure's classification model.

\subsubsection{Bayesian Inference}
Bayesian Inference is a classical technique leveraging Bayes Theorem of
conditional probabilities as stated in equation \ref{eq:BayesTheorum}.
\begin{equation}
    P(A|B) = \frac{P(B | A) P(A)}{P(B)}
    \label{eq:BayesTheorum}
\end{equation}
In Equation \ref{eq:BayesTheorum} we see how to reverse the condition on a conditional probability.
This reversal is precisely the goal of classification in general \autocite{Bishop2009}. Bayesian Inference seemed suitable for CloudAssure, but we continued looking for a methodology that could give better accuracy.

\subsubsection{Decision Trees}
The final technique we surveyed for a classification technique is the Decision
Tree. The Decision Tree divides the search space into a tree. This tree can be
described as a series of nested if statements as in
Algorithm \ref{alg:decision_tree}. Each branch of the
tree prunes a set of features from the search space reducing the features to
check at each decision \autocite{Segaran2008}. This tree structure is
proportional to the size of the training set, and our experiments show it to be
only 15\% smaller than the training set, where the frequency distribution table
for the Bayesian implementation is 84\% smaller than the training set. Despite
the increased memory load for this technique, we found the classification
results to be more accurate than the Bayesian method, and included it in
CloudAssure.

\begin{algorithm}
    \label{alg:decision_tree}
    \caption{Example how the decision tree can be divided into a series of
    nested if statements}
\begin{algorithmic}
    \If{ contains(professor) == False}
        \If{ contains(course) == False}
            \If{ contains(syllabus) == False}
                \If{ contains(faculty) == False} 
                    \Return{ u'Student'}
                \EndIf
                 \If{ contains(faculty) == True} 
                    \Return{ u'Student'}
                \EndIf
                \If{ contains(syllabus) == True}
                        \If{ contains(faculty) == False} 
                            \Return{ u'Course'}
                        \EndIf
                        \If{ contains(faculty) == True} 
                            \Return{ u'Faculty'}
                        \EndIf
                        \If{ contains(course) == True}
                            \If{ contains(interests) == False}
                                \If{ contains(resume) == False} 
                                        \Return{ u'Course'}
                                \EndIf
                            \EndIf
                        \EndIf
                    \EndIf
                \EndIf
            \EndIf
        \EndIf
\end{algorithmic}
\end{algorithm}

\subsection{Implementation}
The Classification System
needs to be properly trained for the best performance \autocite{Russell2008}.
CloudAssure has two possible algorithms available: Bayesian and Decision Tree.
Depending on the implementor's dataset, one will offer more accurate results.

The Classifier, should provide a statically significant improvement over simply
guessing a category. For instance, if the dataset contains 4 categories, then
random chance will classify a document in the correct category 25\% of the time.
To be useful, our classifier must do more accurate job than this.
Table~\ref{tab:classification_accuracy} describes the results for the example
dataset \autocite{University}.

\begin{table}[h!]
    \centering
    \begin{tabular}{c | c }
        \hline
        Classification Method & Accuracy \\
        \hline \hline
        Random Chance & 25\%\\
        Bayesian & 82.54\% \\
        Decision Tree & 86.87\% \\
    \end{tabular}
    \caption{Summary Accuracies of varying classification methods.}
    \label{tab:classification_accuracy}
\end{table}

Our implementation is divided into two phases: A Direct Implementation, and
a second implementation utilizing the \gls{nltk}. The initial implementation was
a direct application of Bayes theorem in Python. This implementation resulted in
several faults however. Performance was poor, and classifying a large number of
documents resulted in floating point underflows, and other implementation
specific faults \autocite{Graham-Cummings2005}. There exists techniques to repair these problems such as
rewriting a products  of terms as a sum of \(log(terms) \). \gls{nltk} however
already implements these techniques in a fast and correct manner. Thus, our results
are based on phase two, which uses \gls{nltk} as a math backend.\autocite{Denoyer2004}

\subsection{Training the Classifiers}
Implementing the Classifier component of CloudAssure in one's facility is broken
into 4 steps:
\begin{enumerate}
    \item Choose a Dataset
    \item Transform the Training Data into features.
    \item Train the 2 provided classifiers to determine which method best
        fits your configuration
    \item Save the plk file for future classifications
\end{enumerate}

\subsubsection{Choosing a DataSet}
The dataset must be categorized into the classifications ones company needs to
classify. It seems pedantic, but the classifiers chooses for CloudAssure cull
new classifications from the data. The classifiers can only categorize data into
given categories. It is critically important that the dataset is evenly distributed across the
categories, and that the definition of each category is not ambiguous. 
Once a dataset is chosen one may begin feature selection.

\subsubsection{Choosing features}
Feature selection is important to disambiguate the categories. The stock feature
selection in CloudAssure uses an industry standard list of stop words to remove
overly common or overly rare words from the features extracted from documents.
Once these words are pruned, choose the top 5000 features. These parameters are tunable
 to get the best results.

\subsubsection{Creating a Database}
Once features have been selected, one may use the python examples to create
a training database. This is done with the following command: 
\texttt{python main.py --learn categoryFolders --name CategoryName}. Python
will recursively search the categoryFolders and extract all features and load
them into a database. Run this step multiple times for as many folders of
training data exist. This step creates a database file used for training the
classifiers.

\subsubsection{Creating a plk file}
Once the database has been built, one may build the persistent classifier. Once
the classifier is trained it is stored in a binary format which allows it to
be reloaded on demand. The following command will load all features from the
database and create a Bayesian model and save it as Bayesian-model.plk: 
\texttt{python main.py --type Bayes --model Bayesian-model.pkl}
Do this also for the decision tree: \texttt{python main.py --type Tree --model
decision-model.pkl}. Each training step will print out the accuracy, and the top
most influential features. This data can be used to tune the classifiers to an organization's needs.
Once the classifier is sufficiently accurate one may use this mode to classify
any document: \texttt{python main.py --classify document.html --name Bayesian-model.pkl}

This is by far the most time intensive step of the process. For our example dataset the tree
model for 3695 documents took 75 minutes (86.87\% accurate) and the Bayes
model for 3695 documents took 3 minutes (82.54\% accurate). This took several
tuning iterations before accuracy levels were sufficient. 

\subsubsection{Security Concerns}
It is critically important that the .plk file be stored in a secure place, or be
cryptographically secured and verified prior to each load. The .plk file, if
manipulated by a maleficent agent, can be used to subvert the trust system and
classify data into any a document to the category of the agent's choosing.  


